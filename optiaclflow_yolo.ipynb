{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32hHzW9WGLI6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
        "print(f\"Pytorch version: {torch.__version__} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYq25YskMOJl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eFAfYgdMR8D"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXvHmGprMWP1"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov7.pt --conf 0.35 --source test_v.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec2xDP5sTJFP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "model = torch.hub.load('WongKinYiu/yolov7', 'custom', \"yolov7.pt\", force_reload=True, trust_repo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiYNIT9tMgOw"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass(frozen=True)\n",
        "class VideoConfig:\n",
        "    fps: float\n",
        "    width: int\n",
        "    height: int\n",
        "\n",
        "def get_video_writer(target_video_path: str, video_config: VideoConfig) -> cv2.VideoWriter:\n",
        "    video_target_dir = os.path.dirname(os.path.abspath(target_video_path))\n",
        "    os.makedirs(video_target_dir, exist_ok=True)\n",
        "    return cv2.VideoWriter(\n",
        "        target_video_path, \n",
        "        fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
        "        fps=video_config.fps, \n",
        "        frameSize=(video_config.width, video_config.height), \n",
        "        isColor=True\n",
        "    )\n",
        "\n",
        "video_config = VideoConfig(\n",
        "    fps=30, \n",
        "    width=1280, \n",
        "    height=720)\n",
        "video_writer = get_video_writer(\n",
        "    target_video_path=\"cica.mp4\", \n",
        "    video_config=video_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5WOnAyZXRPc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from typing import Generator\n",
        "def generate_frames(video_file: str) -> Generator[np.ndarray, None, None]:\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        yield frame\n",
        "\n",
        "    video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlYU5w_0g7CU"
      },
      "outputs": [],
      "source": [
        "frame_iterator = iter(generate_frames(video_file=\"test_v.mp4\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUymAZDVlv38"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "parameters_shitomasi = dict(maxCorners=100, qualityLevel=0.3, minDistance=7)\n",
        "\n",
        "parameters_shitomasi = dict(maxCorners=100, qualityLevel=0.3, minDistance=7)\n",
        "parameter_lucas_kanade = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "for frame in tqdm(frame_iterator):\n",
        "  results = model(frame)\n",
        "  players_center = []\n",
        "  for x_min, y_min, x_max, y_max, confidence, class_id in results.pred[0].cpu().numpy():\n",
        "    print(x_min, y_min, x_max - x_min, y_max - y_min) # width height\n",
        "    print(\"center\", x_min + (x_max - x_min) / 2, y_min + (y_max - y_min) / 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXiQdMKs8FKK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9FrL-DM8FMx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQpjv4C48FPK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O84mIFWv8FRo"
      },
      "outputs": [],
      "source": [
        "%cd yolov7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9JvDay8FTq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "# Create a VideoCapture object to read from the video file\n",
        "cap = cv2.VideoCapture('test_v.mp4')\n",
        "\n",
        "# Read the first frame from the video\n",
        "ret, frame = cap.read()\n",
        "\n",
        "# Select the region of interest (ROI) in the first frame using a bounding box\n",
        "bbox = cv2.selectROI('frame', frame)\n",
        "\n",
        "# Initialize the tracker with the first frame and the ROI\n",
        "tracker = cv2.TrackerLK_create()\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "# Define the video writer to save the output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "# Loop through the video frames and track the object using optical flow\n",
        "while True:\n",
        "    # Read a frame from the video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # If the frame cannot be read, break out of the loop\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Track the object in the current frame using optical flow\n",
        "    ret, bbox = tracker.update(frame)\n",
        "\n",
        "    # If the tracker was successful, draw a bounding box around the tracked object\n",
        "    if ret:\n",
        "        x, y, w, h = [int(i) for i in bbox]\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Write the current frame with the bounding box to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Display the current frame with the bounding box around the tracked object (optional)\n",
        "    # cv2.imshow('frame', frame)\n",
        "\n",
        "    # Wait for a key press and exit if the 'q' key is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the VideoCapture object, the output video, and close all windows (if any)\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw5tuEDpIvCc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7nNALk5IvFS"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FgaW6OrIvIE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "  \n",
        "cap = cv2.VideoCapture('test_v.mp4')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), \n",
        "out = cv2.VideoWriter('output_video.mp4', fourcc=fourcc, fps=fps, frameSize=(width, height), isColor=True) # Change the output file extension to .mp4\n",
        "# params for corner detection\n",
        "feature_params = dict( maxCorners = 100,\n",
        "                       qualityLevel = 0.3,\n",
        "                       minDistance = 7,\n",
        "                       blockSize = 7 )\n",
        "  \n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict( winSize = (15, 15),\n",
        "                  maxLevel = 2,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,\n",
        "                              10, 0.03))\n",
        "  \n",
        "# Create some random colors\n",
        "color = np.random.randint(0, 255, (100, 3))\n",
        "  \n",
        "# Take first frame and find corners in it\n",
        "ret, old_frame = cap.read()\n",
        "old_gray = cv2.cvtColor(old_frame,\n",
        "                        cv2.COLOR_BGR2GRAY)\n",
        "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None,\n",
        "                             **feature_params)\n",
        "  \n",
        "# Create a mask image for drawing purposes\n",
        "mask = np.zeros_like(old_frame)\n",
        "  \n",
        "while(1):\n",
        "      \n",
        "    ret, frame = cap.read()\n",
        "    frame_gray = cv2.cvtColor(frame,\n",
        "                              cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "    # calculate optical flow\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray,\n",
        "                                           frame_gray,\n",
        "                                           p0, None,\n",
        "                                           **lk_params)\n",
        "  \n",
        "    # Select good points\n",
        "    good_new = p1[st == 1]\n",
        "    good_old = p0[st == 1]\n",
        "  \n",
        "    # draw the tracks\n",
        "    for i, (new, old) in enumerate(zip(good_new, \n",
        "                                       good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        mask = cv2.line(mask, (a, b), (c, d),\n",
        "                        color[i].tolist(), 2)\n",
        "          \n",
        "        frame = cv2.circle(frame, (a, b), 5,\n",
        "                           color[i].tolist(), -1)\n",
        "          \n",
        "    img = cv2.add(frame, mask)\n",
        "  \n",
        "    cv2.imshow('frame', img)\n",
        "      \n",
        "    k = cv2.waitKey(25)\n",
        "    if k == 27:\n",
        "        break\n",
        "  \n",
        "    # Updating Previous frame and points \n",
        "    old_gray = frame_gray.copy()\n",
        "    p0 = good_new.reshape(-1, 1, 2)\n",
        "  \n",
        "cv2.destroyAllWindows()\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14fCM_qSae_6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gzIb-oSafCw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3f7s31fafFU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI9rOc8JafHm"
      },
      "outputs": [],
      "source": [
        "# get the code \n",
        "!git clone https://github.com/autonomousvision/unimatch\n",
        "%cd /content/unimatch\n",
        "# dependencies\n",
        "!bash pip_install.sh\n",
        "# check env\n",
        "import torch\n",
        "\n",
        "print('PyTorch version: %s' % torch.__version__)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('Running on CPU')\n",
        "else:\n",
        "  print('Runing on GPU %s' % torch.cuda.get_device_name())\n",
        "\n",
        "# we assume the downloaded model weights are located under the pretrained directory.\n",
        "%mkdir pretrained\n",
        "\n",
        "# we provide a large number of model weights with different speed-accuracy trade-off trained on different datasets for downloading: \n",
        "# https://github.com/autonomousvision/unimatch/blob/master/MODEL_ZOO.md\n",
        "# download example weights for flow, stereo and depth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmflow-scale2-regrefine6-mixdata-train320x576-4e7b215d.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmflow-scale2-regrefine6-kitti15-25b554d7.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmstereo-scale2-regrefine3-resumeflowthings-middleburyfthighres-a82bec03.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmdepth-scale1-regrefine1-resumeflowthings-scannet-90325722.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Kbztz1bHq0"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python main_flow.py \\\n",
        "--inference_video demo/kitti.mp4 \\\n",
        "--resume pretrained/gmflow-scale2-regrefine6-kitti15-25b554d7.pth \\\n",
        "--output_path output/kitti \\\n",
        "--padding_factor 32 \\\n",
        "--upsample_factor 4 \\\n",
        "--num_scales 2 \\\n",
        "--attn_splits_list 2 8 \\\n",
        "--corr_radius_list -1 4 \\\n",
        "--prop_radius_list -1 1 \\\n",
        "--reg_refine \\\n",
        "--num_reg_refine 6 \\\n",
        "--save_video \\\n",
        "--concat_flow_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-FdBIM1irtI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture('test_v.mp4')\n",
        "ret, prev_frame = cap.read()\n",
        "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "img = model(prev_frame)\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7rQml8f-SE7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_172oBJ-SKL"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtF85PLo-VLW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/chuanenlin/optical-flow.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osR9aesq-X9Z"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Parameters for Shi-Tomasi corner detection\n",
        "feature_params = dict(maxCorners = 300, qualityLevel = 0.2, minDistance = 2, blockSize = 7)\n",
        "# Parameters for Lucas-Kanade optical flow\n",
        "lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "# The video feed is read in as a VideoCapture object\n",
        "cap = cv.VideoCapture(\"test_v.mp4\")\n",
        "# Variable for color to draw optical flow track\n",
        "color = (0, 255, 0)\n",
        "# ret = a boolean return value from getting the frame, first_frame = the first frame in the entire video sequence\n",
        "ret, first_frame = cap.read()\n",
        "# Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv.VideoWriter('output.mp4', fourcc, cap.get(cv.CAP_PROP_FPS), (int(cap.get(cv.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))\n",
        "prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
        "mask = np.zeros_like(first_frame)\n",
        "\n",
        "while(cap.isOpened()):\n",
        "    # ret = a boolean return value from getting the frame, frame = the current frame being projected in the video\n",
        "    ret, frame = cap.read()\n",
        "    # Converts each frame to grayscale - we previously only converted the first frame to grayscale\n",
        "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "    next, status, error = cv.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params)\n",
        "\n",
        "    # Selects good feature points for previous position\n",
        "    good_old = prev[status == 1]\n",
        "    # Selects good feature points for next position\n",
        "    good_new = next[status == 1]\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        # Returns a contiguous flattened array as (x, y) coordinates for new point\n",
        "      a, b = new.ravel().astype(int)\n",
        "        # Returns a contiguous flattened array as (x, y) coordinates for old point\n",
        "      c, d = old.ravel().astype(int)\n",
        "        # Draws line between new and old position with green color and 2 thickness\n",
        "      mask = cv.line(mask, (a, b), (c, d), color, 2)\n",
        "        # Draws filled circle (thickness of -1) at new position with green color and radius of 3\n",
        "      frame = cv.circle(frame, (a, b), 3, color, -1)\n",
        "    output = cv.add(frame, mask)\n",
        "    # Visualizes checkpoint 2\n",
        "    # Updates previous frame\n",
        "    prev_gray = gray.copy()\n",
        "    prev = good_new.reshape(-1, 1, 2)\n",
        "\n",
        "    # cv2_imshow(output)\n",
        "    out.write(output)\n",
        "\n",
        "\n",
        "    # Frames are read by intervals of 10 milliseconds. The programs breaks out of the while loop when the user presses the 'q' key\n",
        "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "# The following frees up resources and closes all windows\n",
        "cap.release()\n",
        "out.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH9ep9sf-exz"
      },
      "outputs": [],
      "source": [
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyBeqkQgVIzh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69IWT601VI5A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGIXIfrOVI7s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DwKQ8B6VI-C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# # Load the image\n",
        "img = cv2.imread('test2.jpg')\n",
        "# # Extract the red and near-infrared channels from the image\n",
        "red_channel = img[:,:,1]\n",
        "nir_channel = img[:,:,2]\n",
        "\n",
        "# # Convert the channels to floats and calculate the NDVI\n",
        "red_channel = red_channel.astype(float)\n",
        "nir_channel = nir_channel.astype(float)\n",
        "ndvi = (nir_channel - red_channel) / (nir_channel + red_channel)\n",
        "\n",
        "# # Normalize the NDVI values to the range [0, 255]\n",
        "ndvi_normalized = cv2.normalize(ndvi, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# # Save the NDVI image to a file\n",
        "# cv2.imwrite('ndvi_image.jpg', ndvi)\n",
        "cv2_imshow(ndvi_normalized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8G3yafZWUiy"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Load the RGB image\n",
        "rgb_img = cv2.imread('test.avif')\n",
        "\n",
        "# Convert the RGB image to floating point values between 0 and 1\n",
        "rgb_img_float = rgb_img.astype(np.float32) / 255.0\n",
        "\n",
        "# Calculate an estimate of the NIR reflectance value\n",
        "nir_estimate = (rgb_img_float[:,:,1] + rgb_img_float[:,:,0]) / 2.0\n",
        "\n",
        "# Calculate NDVI\n",
        "ndvi = (nir_estimate - rgb_img_float[:,:,2]) / (nir_estimate + rgb_img_float[:,:,2])\n",
        "\n",
        "# Display the NDVI image\n",
        "cv2_imshow(ndvi)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7149R6ON95gr"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def find_stable_matchings(men_prefs, women_prefs):\n",
        "    \"\"\"\n",
        "    Finds all stable matchings from a set of men and women with preference lists.\n",
        "    \"\"\"\n",
        "    num_people = len(men_prefs)\n",
        "    free_men = set(range(num_people))\n",
        "    unmatched_women = set(range(num_people))\n",
        "    current_matchings = [-1] * num_people  # -1 represents no current matching\n",
        "    proposed_to = [0] * num_people  # index of the last proposal made by each man\n",
        "    women_prefs_ranked = [deque(prefs) for prefs in women_prefs]  # ranked preference list of each woman\n",
        "    \n",
        "    def propose(m):\n",
        "        \"\"\"\n",
        "        Proposes to the highest-ranked woman who has not yet rejected the man.\n",
        "        \"\"\"\n",
        "        w = men_prefs[m][proposed_to[m]]\n",
        "        proposed_to[m] += 1\n",
        "        if current_matchings[w] == -1:\n",
        "            current_matchings[w] = m\n",
        "            free_men.remove(m)\n",
        "            if unmatched_women[w] is not None:\n",
        "              unmatched_women.remove(w)\n",
        "            return True\n",
        "        else:\n",
        "            m_prime = current_matchings[w]\n",
        "            if women_prefs_ranked[w].index(m) < women_prefs_ranked[w].index(m_prime):\n",
        "                current_matchings[w] = m\n",
        "                free_men.remove(m)\n",
        "                free_men.add(m_prime)\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "    \n",
        "    def unpropose(m):\n",
        "        \"\"\"\n",
        "        Removes the current matching between the man and his partner.\n",
        "        \"\"\"\n",
        "        w = current_matchings[m]\n",
        "        current_matchings[m] = -1\n",
        "        free_men.add(m)\n",
        "        unmatched_women.add(w)\n",
        "    \n",
        "    def find_all_matchings():\n",
        "        \"\"\"\n",
        "        Recursively finds all stable matchings using backtracking.\n",
        "        \"\"\"\n",
        "        if not unmatched_women:\n",
        "            yield list(current_matchings)\n",
        "        else:\n",
        "            w = unmatched_women.pop()\n",
        "            for m in women_prefs[w]:\n",
        "                if m in free_men:\n",
        "                    if propose(m):\n",
        "                        yield from find_all_matchings()\n",
        "                        unpropose(m)\n",
        "                elif women_prefs_ranked[w].index(m) < women_prefs_ranked[w].index(current_matchings[m]):\n",
        "                    old_partner = current_matchings[m]\n",
        "                    unpropose(old_partner)\n",
        "                    if propose(m):\n",
        "                        yield from find_all_matchings()\n",
        "                        unpropose(m)\n",
        "                    current_matchings[m] = old_partner\n",
        "                    current_matchings[old_partner] = m\n",
        "    \n",
        "    yield from find_all_matchings()\n",
        "\n",
        "\n",
        "men_prefs = [[1, 0, 2], [0, 1, 2], [2, 1, 0]]\n",
        "women_prefs = [[0, 1, 2], [1, 0, 2], [2, 0, 1]]\n",
        "\n",
        "for matching in find_stable_matchings(men_prefs, women_prefs):\n",
        "    print(matching)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HuWRkN2-ySw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twEAFXXj-yVY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0OZ_aG8-yYl"
      },
      "outputs": [],
      "source": [
        "# get the code \n",
        "!git clone https://github.com/autonomousvision/unimatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1ptWctG-1uh"
      },
      "outputs": [],
      "source": [
        "%cd /content/unimatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qDNCBOf-10R"
      },
      "outputs": [],
      "source": [
        "# dependencies\n",
        "!bash pip_install.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQC9lfRY-5qC"
      },
      "outputs": [],
      "source": [
        "# check env\n",
        "import torch\n",
        "\n",
        "print('PyTorch version: %s' % torch.__version__)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('Running on CPU')\n",
        "else:\n",
        "  print('Runing on GPU %s' % torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoLOZxe1_h0N"
      },
      "outputs": [],
      "source": [
        "# we assume the downloaded model weights are located under the pretrained directory.\n",
        "# %mkdir pretrained# we provide a large number of model weights with different speed-accuracy trade-off trained on different datasets for downloading: \n",
        "# https://github.com/autonomousvision/unimatch/blob/master/MODEL_ZOO.md\n",
        "# download example weights for flow, stereo and depth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmflow-scale2-regrefine6-mixdata-train320x576-4e7b215d.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmflow-scale2-regrefine6-kitti15-25b554d7.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmstereo-scale2-regrefine3-resumeflowthings-middleburyfthighres-a82bec03.pth\n",
        "!wget -P pretrained https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmdepth-scale1-regrefine1-resumeflowthings-scannet-90325722.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvQTyl6RBWG4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "vidcap = cv2.VideoCapture('./unimatch/demo/test_rov.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "  cv2.imwrite(\"./unimatch/demo/foot/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
        "  success,image = vidcap.read()\n",
        "  print('Read a new frame: ', success)\n",
        "  count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFqTq71mh4OM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQOkBuNRCi77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTGFDcKE_le8"
      },
      "outputs": [],
      "source": [
        "# inference on posed images\n",
        "# script from: https://github.com/autonomousvision/unimatch/blob/master/scripts/gmdepth_demo.sh\n",
        "!python main_depth.py \\\n",
        "--inference_dir demo/foot \\\n",
        "--output_path output/foot \\\n",
        "--resume pretrained/gmdepth-scale1-regrefine1-resumeflowthings-scannet-90325722.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXiaxwYMKo95"
      },
      "outputs": [],
      "source": [
        "# inference on video\n",
        "# script from: https://github.com/autonomousvision/unimatch/blob/master/scripts/gmflow_demo.sh\n",
        "!python main_flow.py \\\n",
        "--inference_video demo/test_rov.mp4 \\\n",
        "--resume pretrained/gmflow-scale2-regrefine6-kitti15-25b554d7.pth \\\n",
        "--output_path output/foci \\\n",
        "--padding_factor 32 \\\n",
        "--upsample_factor 4 \\\n",
        "--num_scales 2 \\\n",
        "--attn_splits_list 2 8 \\\n",
        "--corr_radius_list -1 4 \\\n",
        "--prop_radius_list -1 1 \\\n",
        "--reg_refine \\\n",
        "--num_reg_refine 6 \\\n",
        "--save_video \\\n",
        "--concat_flow_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53f-wVR_M3ja"
      },
      "outputs": [],
      "source": [
        "%rm -rf output/foci\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmkvj4mBSAfP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Set the directory containing the frames\n",
        "frame_dir = 'output/foci'\n",
        "\n",
        "# Set the output video file name and frame rate\n",
        "output_file = 'output.mp4'\n",
        "fps = 30\n",
        "\n",
        "# Get the list of frames in the directory\n",
        "frames = os.listdir(frame_dir)\n",
        "frames.sort()\n",
        "\n",
        "# Set the video dimensions to match the first frame\n",
        "frame_path = os.path.join(frame_dir, frames[0])\n",
        "frame = cv2.imread(frame_path)\n",
        "height, width, _ = frame.shape\n",
        "\n",
        "# Create the video writer object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
        "\n",
        "# Write each frame to the video file\n",
        "for frame_name in frames:\n",
        "    frame_path = os.path.join(frame_dir, frame_name)\n",
        "    frame = cv2.imread(frame_path)\n",
        "    out.write(frame)\n",
        "\n",
        "# Release the video writer object\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXwWLlR-ZtWO"
      },
      "outputs": [],
      "source": [
        "!pip install flow_vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRBy5TxTZuXB"
      },
      "outputs": [],
      "source": [
        "!pip install globalVariables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_csquZza0lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsBllPrnawvK"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/princeton-vl/RAFT.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQDq0WB4bLhA"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/4j4z58wuv8o0mfz/models.zip\n",
        "!unzip models.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMIxUTjWbqr4"
      },
      "outputs": [],
      "source": [
        "pip install RAFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVtMpOYTbtVd"
      },
      "outputs": [],
      "source": [
        "!python RAFT/core/raft.py --model=models/raft-things.pth --input=test_rov.mp4 --output=output_flow.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8lKs1_kh5_E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3m23rSeh6CB"
      },
      "outputs": [],
      "source": [
        "%cd RAFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QcvSx8Kjf1T"
      },
      "outputs": [],
      "source": [
        "pip install pytorch=1.6.0 torchvision=0.7.0 cudatoolkit=10.1 matplotlib tensorboard scipy opencv -c pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LCo7ODviCQc"
      },
      "outputs": [],
      "source": [
        "!python demo.py --model=./models/raft-things.pth --path=demo-frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7CtNCZok1Lg"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/foot.zip /content/unimatch/demo/foot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVuX5QValLtc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/foot.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC9QMz358d7T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJ5EzUY8d9w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZWZkfzF8eAI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/uzh-rpg/E-RAFT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKePeBpX8fr3"
      },
      "outputs": [],
      "source": [
        "%cd E-RAFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zNeY3crLVOmB"
      },
      "outputs": [],
      "source": [
        "!python download_dsec_test.py OUTPUT_DIRECTORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58Szd3ORuSla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l7UXaGhuStf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP_IDTqHuSwC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Load two consecutive frames\n",
        "frame1 = cv2.imread('frame0.jpg')\n",
        "frame2 = cv2.imread('frame1.jpg')\n",
        "\n",
        "# Convert frames to grayscale\n",
        "prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "next_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Calculate dense optical flow using Farneback method\n",
        "flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "# Compute magnitude and angle of the optical flow vectors\n",
        "magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "# Convert angle to hue, and magnitude to value in HSV color space\n",
        "hsv = np.zeros_like(frame1)\n",
        "hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "hsv[..., 1] = 255\n",
        "hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# Convert HSV to BGR and show the resulting image\n",
        "flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "cv2_imshow(flow_rgb)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIw9FS4gu235"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture('test_rov.mp4')\n",
        "\n",
        "# Read first frame\n",
        "ret, frame1 = cap.read()\n",
        "\n",
        "# Convert first frame to grayscale\n",
        "prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Define parameters for Farneback method\n",
        "pyr_scale = 0.5\n",
        "levels = 3\n",
        "winsize = 15\n",
        "iterations = 3\n",
        "poly_n = 5\n",
        "poly_sigma = 1.2\n",
        "flags = 0\n",
        "\n",
        "# Process video frame by frame\n",
        "while True:\n",
        "    # Read next frame\n",
        "    ret, frame2 = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert next frame to grayscale\n",
        "    next_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate dense optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)\n",
        "\n",
        "    # Compute magnitude and angle of the optical flow vectors\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Convert angle to hue, and magnitude to value in HSV color space\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "    hsv[..., 1] = 255\n",
        "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Convert HSV to BGR and show the resulting image\n",
        "    flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    cv2_imshow(flow_rgb)\n",
        "\n",
        "    # Wait for key press and break if 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    # Update previous frame\n",
        "    prev_gray = next_gray\n",
        "\n",
        "# Release video capture and close all windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2bRaoD2vMyQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture('test_v.mp4')\n",
        "\n",
        "# Define output video properties\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "# Create output video writer\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
        "\n",
        "# Read first frame\n",
        "ret, frame1 = cap.read()\n",
        "\n",
        "# Convert first frame to grayscale\n",
        "prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Define parameters for Farneback method\n",
        "pyr_scale = 0.5\n",
        "levels = 3\n",
        "winsize = 15\n",
        "iterations = 3\n",
        "poly_n = 5\n",
        "poly_sigma = 1.2\n",
        "flags = 0\n",
        "\n",
        "# Process video frame by frame\n",
        "while True:\n",
        "    # Read next frame\n",
        "    ret, frame2 = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert next frame to grayscale\n",
        "    next_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    matrix = np.zeros_like(frame2, dtype=np.uint8)\n",
        "\n",
        "    # Calculate dense optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)\n",
        "\n",
        "    # Compute magnitude and angle of the optical flow vectors\n",
        "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Convert angle to hue, and magnitude to value in HSV color space\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 0] = angle * 180 / np.pi / 2\n",
        "    hsv[..., 1] = 255\n",
        "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Convert HSV to BGR and overlay the resulting flow image onto the original frame\n",
        "    flow_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    overlay = cv2.addWeighted(frame2, 0.5, flow_bgr, 0.5, 0)\n",
        "\n",
        "    # Write the resulting frame to the output video\n",
        "    out.write(overlay)\n",
        "\n",
        "    # Show the resulting frame in a window\n",
        "    # cv2.imshow('Optical flow', overlay)\n",
        "\n",
        "    # Wait for key press and break if 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    # Update previous frame\n",
        "    prev_gray = next_gray\n",
        "\n",
        "# Release video capture, writer, and close all windows\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9UDOR7gJ6BQ"
      },
      "outputs": [],
      "source": [
        "merge with yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lk8wxj7aSCjb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture('test_rov.mp4')\n",
        "#model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "(W, H) = (None, None)\n",
        "# Define output video properties\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "# Create output video writer\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
        "\n",
        "# Read first frame\n",
        "ret, frame1 = cap.read()\n",
        "\n",
        "res = model(frame1)\n",
        "\n",
        "# Convert first frame to grayscale\n",
        "prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Define parameters for Farneback method\n",
        "pyr_scale = 0.5\n",
        "levels = 3\n",
        "winsize = 15\n",
        "iterations = 3\n",
        "poly_n = 5\n",
        "poly_sigma = 1.2\n",
        "flags = 0\n",
        "\n",
        "# Process video frame by frame\n",
        "while True:\n",
        "    # Read next frame\n",
        "    ret, frame2 = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    if W is None or H is None:\n",
        "        (H, W) = frame2.shape[:2]\n",
        "    # Convert next frame to grayscale\n",
        "    next_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
        "    matrix = np.zeros_like(frame2, dtype=np.uint8)\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    # model prediction\n",
        "    pred = model(frame2)\n",
        "    for output in pred.xyxy[0].cpu():\n",
        "        if float(output[4]) > 0.5:\n",
        "          box = output[0:4] #* np.array([W, H, W, H])\n",
        "          (centerX, centerY, width, height) = box.numpy().astype(\"int\")\n",
        "          x = int(centerX)\n",
        "          y = int(centerY)\n",
        "          # x = int(centerX - (width / 2))\n",
        "          # y = int(centerY - (height / 2))\n",
        "          boxes.append([x, y, int(width), int(height)])\n",
        "          confidences.append(float(output[4]))\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
        "  \n",
        "    if len(idxs) > 0:\n",
        "        for i in idxs.flatten():\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            color = (0, 255, 0)  # green\n",
        "            cv2.rectangle(frame2, (x, y), (x + w, y + h), color, 2)\n",
        "            text = \"{}\".format('Object')\n",
        "            cv2.putText(frame2, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "            \n",
        "            # Apply optical flow to the bounding box region\n",
        "            # print(prev_gray[y:y+h, x:x+w])\n",
        "            roi_gray = prev_gray[y:y+h, x:x+w]\n",
        "            roi_next_gray = next_gray[y:y+h, x:x+w]\n",
        "            # print(x)\n",
        "            # print(y)\n",
        "            # print(roi_gray)\n",
        "            # print(roi_next_gray)\n",
        "            flow = cv2.calcOpticalFlowFarneback(roi_gray, roi_next_gray, None, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)\n",
        "            \n",
        "            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "            hsv = np.zeros_like(roi_gray)\n",
        "\n",
        "            hsv[..., 0] = (angle * 180 / np.pi / 2)[..., 0] #angle * 180 / np.pi / 2\n",
        "            hsv[..., 1] = 255\n",
        "            hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)[..., 2] # cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "            # print(hsv)\n",
        "            # flow_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "            flow_bgr = cv2.cvtColor(cv2.merge([hsv, hsv, hsv]), cv2.COLOR_HSV2BGR)\n",
        "\n",
        "            overlay = cv2.addWeighted(frame2[y:y+h, x:x+w], 0.5, flow_bgr, 0.5, 0)\n",
        "            frame2[y:y+h, x:x+w] = overlay\n",
        "# Write the resulting frame to the output video\n",
        "    out.write(frame2)\n",
        "    # cv2_imshow(frame2)\n",
        "    # Show the resulting frame in a window\n",
        "    # cv2.imshow('Optical flow', frame2)\n",
        "    # Wait for key press and break if 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    # Update previous frame\n",
        "    prev_gray = next_gray\n",
        "# Release video capture, writer, and close all windows\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB_WmlBH4iQl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJAlwLph4iTU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYXQY5u84iYL"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAs42ZnECfxE"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwUADvyfDDb3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDqN1KfsDDfQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j26YpbSKDDh3"
      },
      "outputs": [],
      "source": [
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "    polygons = []\n",
        "    color = []\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
        "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
        "        for i in range(3):\n",
        "            img[:,:,i] = color_mask[i]\n",
        "        ax.imshow(np.dstack((img, m*0.35)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_MFYR724jsC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4MiKfxs46iY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67N-P5CrE8xs"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ6PZJv3INmC"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture('test_rov.mp4')\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
        "\n",
        "ret, frame1 = cap.read()\n",
        "\n",
        "cv2.imwrite(\"frame.png\", frame1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht92Ohomiw1z"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('frame.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFRYMAk9FFuy"
      },
      "outputs": [],
      "source": [
        "\n",
        "masks = mask_generator.generate(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61L28UzNFgYV"
      },
      "outputs": [],
      "source": [
        "print(len(masks))\n",
        "print(masks[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgtnKiUgFizL"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bFWf0AwGLMy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hkchengrex/XMem.git\n",
        "%cd XMem\n",
        "!pip install opencv-python\n",
        "!pip install -U numpy\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSaG3x27GPC6"
      },
      "outputs": [],
      "source": [
        "!wget -P ./saves/ https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8otRLEruuD9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKUTrCktGSzt"
      },
      "outputs": [],
      "source": [
        "# %cd ..\n",
        "import os\n",
        "from os import path\n",
        "from argparse import ArgumentParser\n",
        "import shutil\n",
        "device='cuda'\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from inference.data.test_datasets import LongTestDataset, DAVISTestDataset, YouTubeVOSTestDataset\n",
        "from inference.data.mask_mapper import MaskMapper\n",
        "from model.network import XMem\n",
        "from inference.inference_core import InferenceCore\n",
        "\n",
        "from progressbar import progressbar\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# default configuration\n",
        "config = {\n",
        "    'top_k': 30,\n",
        "    'mem_every': 5,\n",
        "    'deep_update_every': -1,\n",
        "    'enable_long_term': True,\n",
        "    'enable_long_term_count_usage': True,\n",
        "    'num_prototypes': 128,\n",
        "    'min_mid_term_frames': 5,\n",
        "    'max_mid_term_frames': 10,\n",
        "    'max_long_term_elements': 10000,\n",
        "}\n",
        "\n",
        "network = XMem(config, './XMem/saves/XMem.pth').eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYytOkRFs0zx"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdOCVZqwGkph"
      },
      "outputs": [],
      "source": [
        "# %cd content/\n",
        "video_name = 'test_rov.mp4'\n",
        "mask_name = 'kep.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAmxomEjqCgb"
      },
      "outputs": [],
      "source": [
        "# Image.open(mask_name)\n",
        "# gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9J045yGGTZw"
      },
      "outputs": [],
      "source": [
        "mask = np.array(Image.open(mask_name))\n",
        "print(np.unique(mask))\n",
        "num_objects = len(np.unique(mask)) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqvq7npjIDGQ"
      },
      "outputs": [],
      "source": [
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU6XZvzgIDSU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "processor = InferenceCore(network, config=config)\n",
        "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
        "cap = cv2.VideoCapture(video_name)\n",
        "\n",
        "# You can change these two numbers\n",
        "frames_to_propagate = 200\n",
        "visualize_every = 20\n",
        "\n",
        "current_frame_index = 0\n",
        "\n",
        "# mask = mask[..., :3]\n",
        "\n",
        "with torch.cuda.amp.autocast(enabled=True):\n",
        "  while (cap.isOpened()):\n",
        "    # load frame-by-frame\n",
        "    _, frame = cap.read()\n",
        "    if frame is None or current_frame_index > frames_to_propagate:\n",
        "      break\n",
        "\n",
        "    # convert numpy array to pytorch tensor format\n",
        "    frame_torch, _ = image_to_torch(frame, device=device)\n",
        "    if current_frame_index == 0:\n",
        "      # initialize with the mask\n",
        "      \n",
        "      \n",
        "      mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
        "      # the background mask is not fed into the model\n",
        "      prediction = processor.step(frame_torch, mask_torch[1:])\n",
        "    else:\n",
        "      # propagate only\n",
        "      prediction = processor.step(frame_torch)\n",
        "\n",
        "    # argmax, convert to numpy\n",
        "    prediction = torch_prob_to_numpy_mask(prediction)\n",
        "\n",
        "    if current_frame_index % visualize_every == 0:\n",
        "      visualization = overlay_davis(frame, prediction)\n",
        "      display(Image.fromarray(visualization))\n",
        "\n",
        "    current_frame_index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GciqMJP3bXVC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ-1fedHbXXr"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import threading\n",
        "\n",
        "# Create a VideoCapture object for each video\n",
        "cap1 = cv2.VideoCapture('output_1.mp4')\n",
        "cap2 = cv2.VideoCapture('output_2.mp4')\n",
        "cap3 = cv2.VideoCapture('output_3.mp4')\n",
        "cap4 = cv2.VideoCapture('output_4.mp4')\n",
        "\n",
        "# Set the width and height of the video frames\n",
        "frame_width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Create a window to display the videos\n",
        "cv2.namedWindow('Videos', cv2.WINDOW_NORMAL)\n",
        "\n",
        "# Define a function to play a video in a separate thread\n",
        "def play_video(cap):\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        cv2.imshow('Videos', frame)\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "            break\n",
        "\n",
        "# Create a separate thread to play each video\n",
        "thread1 = threading.Thread(target=play_video, args=(cap1,))\n",
        "thread2 = threading.Thread(target=play_video, args=(cap2,))\n",
        "thread3 = threading.Thread(target=play_video, args=(cap3,))\n",
        "thread4 = threading.Thread(target=play_video, args=(cap4,))\n",
        "\n",
        "# Start the threads\n",
        "thread1.start()\n",
        "thread2.start()\n",
        "thread3.start()\n",
        "thread4.start()\n",
        "\n",
        "# Wait for the threads to finish\n",
        "thread1.join()\n",
        "thread2.join()\n",
        "thread3.join()\n",
        "thread4.join()\n",
        "\n",
        "# Release the VideoCapture objects and destroy the window\n",
        "cap1.release()\n",
        "cap2.release()\n",
        "cap3.release()\n",
        "cap4.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2VqTtrBXTsr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UMeCFeRXTvv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvZ_5UroXTyA"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CWxwc9EXYz_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5s.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv9-rlR1XZed"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('test_rov.mp4')\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    results = model(frame, size=416)\n",
        "    print(results.pred)\n",
        "    print(results.names)\n",
        "    print(results.)\n",
        "    break\n",
        "    # ball_results = results.xyxy[results.names.index('football')]\n",
        "    # if len(ball_results) > 0:\n",
        "        # bbox = ball_results[0][0:4]\n",
        "        # cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
        "    # cv2.imshow('frame', frame)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jZthWQbnz-7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecc1FUcEn0Bi"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "cap = cv2.VideoCapture('video.mp4')\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "# out = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)\n",
        "\n",
        "ret, frame1 = cap.read()\n",
        "\n",
        "cv2.imwrite(\"frame_elso.png\", frame1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fyxJEpc7Ati"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49LSabl9n0D7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Load the image\n",
        "img = cv2.imread('football.png')\n",
        "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
        "classes = []\n",
        "with open('coco.names', 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Define the input size of the YOLO model\n",
        "input_size = (416, 416)\n",
        "\n",
        "# Resize the image to the input size of the YOLO model\n",
        "resized_img = cv2.resize(img, input_size)\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "cv2_imshow(gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxqpdYaTpUgo"
      },
      "outputs": [],
      "source": [
        "# Initialize the SIFT feature detector\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "ret, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Detect keypoints and extract descriptors\n",
        "kp, des = sift.detectAndCompute(gray, mask)\n",
        "\n",
        "# Draw the keypoints on the image\n",
        "# img_kp = cv2.drawKeypoints(img, kp, None)\n",
        "img_kp = cv2.drawKeypoints(img, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "# Display the image with keypoints\n",
        "cv2_imshow(img_kp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CIRiin3pqPM"
      },
      "outputs": [],
      "source": [
        "#XMEM hasznald itt\n",
        "!git clone https://github.com/hkchengrex/XMem.git\n",
        "%cd XMem\n",
        "!pip install opencv-python\n",
        "!pip install -U numpy\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQrID0hTA6Lz"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DKwzjyz_naj"
      },
      "outputs": [],
      "source": [
        "%cd XMem\n",
        "!pip install opencv-python\n",
        "!pip install -U numpy\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_BV8hkaQh3z"
      },
      "outputs": [],
      "source": [
        "!wget -P ./saves/ https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8tllfHVQxq4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('Using GPU')\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print('CUDA not available. Please connect to a GPU instance if possible.')\n",
        "  device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbJbV1qrQk70"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import path\n",
        "from argparse import ArgumentParser\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from inference.data.test_datasets import LongTestDataset, DAVISTestDataset, YouTubeVOSTestDataset\n",
        "from inference.data.mask_mapper import MaskMapper\n",
        "from model.network import XMem\n",
        "from inference.inference_core import InferenceCore\n",
        "\n",
        "from progressbar import progressbar\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# default configuration\n",
        "config = {\n",
        "    'top_k': 30,\n",
        "    'mem_every': 5,\n",
        "    'deep_update_every': -1,\n",
        "    'enable_long_term': True,\n",
        "    'enable_long_term_count_usage': True,\n",
        "    'num_prototypes': 128,\n",
        "    'min_mid_term_frames': 5,\n",
        "    'max_mid_term_frames': 10,\n",
        "    'max_long_term_elements': 10000,\n",
        "}\n",
        "\n",
        "network = XMem(config, './saves/XMem.pth').eval().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1ZTAcByQpT5"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget -O video.mp4 https://user-images.githubusercontent.com/7107196/177661140-f690156b-1775-4cd7-acd7-1738a5c92f30.mp4\n",
        "!wget -O first_frame.png https://i.imgur.com/3ueaiBA.png\n",
        "\n",
        "video_name = 'video.mp4'\n",
        "mask_name = 'first_frame.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KndCH_QQrkw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(open(video_name, 'rb').read()).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMdDYr5FQ2B2"
      },
      "outputs": [],
      "source": [
        "import IPython.display\n",
        "IPython.display.Image('first_frame.png', width=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFcGPNeCQ6no"
      },
      "outputs": [],
      "source": [
        "#nocvert mask to np array\n",
        "mask = np.array(Image.open(mask_name))\n",
        "print(np.unique(mask))\n",
        "num_objects = len(np.unique(mask)) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cqe2k69vQsl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load mask image\n",
        "mask = np.array(Image.open(mask_name))\n",
        "\n",
        "# Get unique pixel values in the mask\n",
        "unique_values = np.unique(mask)\n",
        "\n",
        "# Set the color map\n",
        "cmap = plt.cm.get_cmap('jet', len(unique_values))\n",
        "\n",
        "# Visualize the mask array using the color map\n",
        "plt.imshow(mask, cmap=cmap, vmin=unique_values[0], vmax=unique_values[-1])\n",
        "plt.colorbar()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV1kzmCVYDOu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def distance(x1, y1, x2, y2):\n",
        "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
        "\n",
        "def perimeter_of_quadrilateral(a, b, c, d):\n",
        "    x1, y1 = a\n",
        "    x2, y2 = b\n",
        "    x3, y3 = c\n",
        "    x4, y4 = d\n",
        "    \n",
        "    AB = distance(x1, y1, x2, y2)\n",
        "    BC = distance(x2, y2, x3, y3)\n",
        "    CD = distance(x3, y3, x4, y4)\n",
        "    DA = distance(x4, y4, x1, y1)\n",
        "    \n",
        "    perimeter = AB + BC + CD + DA\n",
        "    \n",
        "    return perimeter\n",
        "\n",
        "arr = np.array([\n",
        "    [0,0,4,4,4,0,0,0],\n",
        "    [0,0,4,4,4,4,4,0],\n",
        "    [0,4,4,4,4,4,0,0],\n",
        "    [0,1,1,1,1,1,1,1],\n",
        "    [1,1,1,1,1,0,0,0],\n",
        "    [0,0,1,0,1,1,1,0]\n",
        "])\n",
        "# expected output row: first oc 0 and last oc: 2\n",
        "# expected output closest to zero is [2,1] closest to the end of the row: [1,6]\n",
        "\n",
        "# Find the (x, y) coordinates of the first occurrence of 1\n",
        "num_object = [1,4]\n",
        "for obj_id in num_object:\n",
        "  first_occurrence_idx = np.argmax(arr == obj_id)\n",
        "  first_occurrence_coordinates = np.unravel_index(first_occurrence_idx, arr.shape)\n",
        "\n",
        "  # Find the (x, y) coordinates of the last occurrence of 1\n",
        "  last_occurrence_idx = np.argwhere(arr == obj_id)[-1]\n",
        "  last_occurrence_coordinates = tuple(last_occurrence_idx)\n",
        "\n",
        "  # so far so good\n",
        "\n",
        "  coordinates = np.argwhere(arr == obj_id)\n",
        "\n",
        "  min_y_idx_in_coordinates = np.argmin(coordinates[:, 1])\n",
        "  max_y_idx_in_coordinates = np.argmax(coordinates[:, 1])\n",
        "\n",
        "  min_y_element = coordinates[min_y_idx_in_coordinates]\n",
        "  max_y_element = coordinates[max_y_idx_in_coordinates]\n",
        "\n",
        "  # Calculate the distances from the 0-th index\n",
        "\n",
        "  print(\"id\", obj_id)\n",
        "  print(f\"First occurrence coordinates: {first_occurrence_coordinates}\")\n",
        "  print(f\"Last occurrence coordinates: {last_occurrence_coordinates}\")\n",
        "\n",
        "  print(\"Smallest y element:\", min_y_element)\n",
        "  print(\"Largest y element:\", max_y_element)\n",
        "\n",
        "  print(first_occurrence_coordinates, last_occurrence_coordinates,min_y_element,max_y_element )\n",
        "  a,b,c,d = first_occurrence_coordinates, last_occurrence_coordinates, min_y_element, max_y_element\n",
        "  #  kiveszem az x min es y min es kiveszem a max x es max y?\n",
        "  max_x = max(a[0], b[0], c[0], d[0])\n",
        "  min_x = min(a[0], b[0], c[0], d[0])\n",
        "  max_y = max(a[1], b[1], c[1], d[1])\n",
        "  min_y = min(a[1], b[1], c[1], d[1])\n",
        "\n",
        "  print(\"Max x value:\", max_x)\n",
        "  print(\"Min x value:\", min_x)\n",
        "  print(\"Max y value:\", max_y)\n",
        "  print(\"Min y value:\", min_y)\n",
        "def area_of_quadrilateral(a, b, c, d):\n",
        "    x1, y1 = a\n",
        "    x2, y2 = b\n",
        "    x3, y3 = c\n",
        "    x4, y4 = d\n",
        "    \n",
        "    area = 0.5 * abs((x1*y2 + x2*y3 + x3*y4 + x4*y1) - (y1*x2 + y2*x3 + y3*x4 + y4*x1))\n",
        "    \n",
        "    return area\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXMmsRDQli33"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def distance(x1, y1, x2, y2):\n",
        "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
        "\n",
        "def perimeter_of_quadrilateral(a, b, c, d):\n",
        "    x1, y1 = a\n",
        "    x2, y2 = b\n",
        "    x3, y3 = c\n",
        "    x4, y4 = d\n",
        "    \n",
        "    AB = distance(x1, y1, x2, y2)\n",
        "    BC = distance(x2, y2, x3, y3)\n",
        "    CD = distance(x3, y3, x4, y4)\n",
        "    DA = distance(x4, y4, x1, y1)\n",
        "    \n",
        "    perimeter = AB + BC + CD + DA\n",
        "    \n",
        "    return perimeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HykBWyYRBYJ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "processor = InferenceCore(network, config=config)\n",
        "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
        "cap = cv2.VideoCapture(video_name)\n",
        "\n",
        "# You can change these two numbers\n",
        "frames_to_propagate = 200\n",
        "visualize_every = 20\n",
        "\n",
        "current_frame_index = 0\n",
        "\n",
        "with torch.cuda.amp.autocast(enabled=True):\n",
        "  while (cap.isOpened()):\n",
        "    # load frame-by-frame\n",
        "    _, frame = cap.read()\n",
        "    if frame is None or current_frame_index > frames_to_propagate:\n",
        "      break\n",
        "\n",
        "    # convert numpy array to pytorch tensor format\n",
        "    frame_torch, _ = image_to_torch(frame, device=device)\n",
        "    if current_frame_index == 0:\n",
        "      # initialize with the mask\n",
        "      mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
        "      # the background mask is not fed into the model\n",
        "      prediction = processor.step(frame_torch, mask_torch[1:])\n",
        "    else:\n",
        "      # propagate only\n",
        "      prediction = processor.step(frame_torch)\n",
        "\n",
        "\n",
        "    # print(\"elso\",prediction)\n",
        "    # argmax, convert to numpy\n",
        "    prediction = torch_prob_to_numpy_mask(prediction)\n",
        "    # x_s, y_s = prediction.shape\n",
        "    # for x in range(x_s):\n",
        "    #   for y in range(y_s):\n",
        "    #     print(prediction[x,y], end=\"\")\n",
        "    #   print(end=\"\\n\")\n",
        "\n",
        "    a,b,c,d = None, None, None, None\n",
        "    permit = math.inf\n",
        "    for obj_id in np.unique(mask)[1:]:\n",
        "      first_occurrence_idx = np.argmax(prediction == obj_id)\n",
        "      first_occurrence_coordinates = np.unravel_index(first_occurrence_idx, prediction.shape)\n",
        "\n",
        "      # Find the (x, y) coordinates of the last occurrence of 1\n",
        "      last_occurrence_idx = np.argwhere(prediction == obj_id)[-1]\n",
        "      last_occurrence_coordinates = tuple(last_occurrence_idx)\n",
        "\n",
        "      # so far so good\n",
        "\n",
        "      coordinates = np.argwhere(prediction == obj_id)\n",
        "\n",
        "      min_y_idx_in_coordinates = np.argmin(coordinates[:, 1])\n",
        "      max_y_idx_in_coordinates = np.argmax(coordinates[:, 1])\n",
        "\n",
        "      min_y_element = coordinates[min_y_idx_in_coordinates]\n",
        "      max_y_element = coordinates[max_y_idx_in_coordinates]\n",
        "\n",
        "      # Calculate the distances from the 0-th index\n",
        "\n",
        "      # print(\"id\", obj_id)\n",
        "      # print(f\"First occurrence coordinates: {first_occurrence_coordinates}\")\n",
        "      # print(f\"Last occurrence coordinates: {last_occurrence_coordinates}\")\n",
        "\n",
        "      # print(\"Smallest y element:\", min_y_element)\n",
        "      # print(\"Largest y element:\", max_y_element)\n",
        "      quadrilateral = perimeter_of_quadrilateral(first_occurrence_coordinates, last_occurrence_coordinates, min_y_element, max_y_element)\n",
        "      # print(\"permiter\", quadrilateral)\n",
        "\n",
        "      if quadrilateral < permit:\n",
        "        a,b,c,d = first_occurrence_coordinates, last_occurrence_coordinates, min_y_element, max_y_element\n",
        "        permit = quadrilateral\n",
        "      # print(a,b,c,d)\n",
        "    max_x = max(a[0], b[0], c[0], d[0])\n",
        "    max_y = max(a[1], b[1], c[1], d[1])\n",
        "    min_x = min(a[0], b[0], c[0], d[0])\n",
        "    min_y = min(a[1], b[1], c[1], d[1])\n",
        "      # print(tuple([min_x,min_y]), tuple([max_x,max_y]))\n",
        "      # print(tuple([min_x,max_y]),tuple([max_x,min_y]))\n",
        "    # print(tuple([max_y, min_x]),tuple([min_y, max_x]))\n",
        "    frame_bbox = cv2.rectangle(frame,tuple([max_y, min_x]),tuple([min_y, max_x]), (0, 255, 0), 2)\n",
        "      \n",
        "    if current_frame_index % visualize_every == 0:\n",
        "      visualization = overlay_davis(frame_bbox, prediction)\n",
        "      display(Image.fromarray(visualization))\n",
        "\n",
        "    current_frame_index += 1\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_4XH-X57QA8"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWy2zoVF7EOm"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AizVhEbT70WK"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsyYPTCr77DP"
      },
      "outputs": [],
      "source": [
        "!wget \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa59hgqt8IRW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA2UbYoz8YGR"
      },
      "outputs": [],
      "source": [
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK1yPOs_8i4C"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('frame_elso.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSGma5M9XOI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGOpzcRI9duU"
      },
      "outputs": [],
      "source": [
        "masks = mask_generator.generate(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr-mmGjw9gi4"
      },
      "outputs": [],
      "source": [
        "print(len(masks))\n",
        "print(masks[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMYMogHQ9iE9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhMz7X7X_9sx"
      },
      "source": [
        "# The collab can't hold this many masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jukoXQ30AAHK"
      },
      "outputs": [],
      "source": [
        "# select frame instead\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "    \n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
        "    \n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG7-ML49Ad-8"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KZgZieiBHo3"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fOhyomHAaoc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "image = cv2.imread('frame_elso.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('on')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eJ88HKoBnGX"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83TeWgL5BSbI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bODtKE0oBa7i"
      },
      "outputs": [],
      "source": [
        "  predictor.set_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yblGEBXrBb7e"
      },
      "outputs": [],
      "source": [
        "input_point = np.array([[799, 553]])\n",
        "input_label = np.array([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeiO4J3mCAeh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('on')\n",
        "plt.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHz2JE_tCCtr"
      },
      "outputs": [],
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLKFDbZGCIlv"
      },
      "outputs": [],
      "source": [
        "dpi = 96\n",
        "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    # plt.figure(figsize=(10,10))\n",
        "    fig = plt.figure(figsize=(1920/dpi, 1080/dpi), dpi=dpi)\n",
        "    plt.imshow(image)\n",
        "    show_mask(mask, plt.gca())\n",
        "    # show_points(input_point, input_label, plt.gca())\n",
        "    # plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "    plt.axis('off')\n",
        "    # plt.show()  \n",
        "    plt.savefig('foo.png',bbox_inches='tight')\n",
        "    break\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "nmERB2EDEyC_",
        "outputId": "de8bc5d7-8eaf-4896-9eab-c0012ef0dcca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB4AAAAQ4CAAAAADNuJ6fAAAIZElEQVR4nO3bMQ6EIBBAUd373xm73cCYlcTBsXivko5m+MGEbQMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAOnv1BuCl2u/TmAD5nCxwpnUrcwKk+1RvAN6o/V0C3CfAEAkusJwAQxD7q8hANgGGkdoCDxBgmCHKQDIBBoACAgwDl13gCQIMM7wEBpIJMAAUEGAAKCDAMMEfaCCbAMM1/QXSCTAMYm31F8gnwHBFf4EFHC0QdC+BzQiwhMMFTnwTbEIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG44AFqxDB9gBih2AAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=1920x1080 at 0x7F9D8E638700>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Open image\n",
        "img = Image.open(\"frame_elso.png\")\n",
        "\n",
        "\n",
        "# Create mask around image\n",
        "img_mask = Image.fromarray((mask * 255).astype(np.uint8), mode='L')\n",
        "img_mask = img_mask.crop(img.getbbox())\n",
        "\n",
        "# Show mask\n",
        "img_mask.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBAU7IoZFIeN"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4o49vC-z7O",
        "outputId": "33ea39cd-0e38-4f88-be13-45b89b6d3536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/XMem\n"
          ]
        }
      ],
      "source": [
        "%cd XMem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Cip4o1oq-DsR"
      },
      "outputs": [],
      "source": [
        "video_name = 'video.mp4'\n",
        "mask_name = 'first_frame.png'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "742Uo2FC-5-p"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(open(video_name, 'rb').read()).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn7eHqtY-gdM",
        "outputId": "0ef4a363-9218-46b3-d279-25ec759fb408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "[  0 255]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#nocvert mask to np array\n",
        "mask = np.array(Image.open(mask_name))\n",
        "print(mask)\n",
        "print(np.unique(mask))\n",
        "num_objects = len(np.unique(mask)) - 1\n",
        "print(num_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "S0gIPT1w-BUI",
        "outputId": "567a6b72-65d9-4236-c8eb-db0df882cf7b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9ba1c5f765b5>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_frame_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;31m# initialize with the mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mmask_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_numpy_to_one_hot_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_objects\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;31m# the background mask is not fed into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_torch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/XMem/inference/interact/interactive_utils.py\u001b[0m in \u001b[0;36mindex_numpy_to_one_hot_torch\u001b[0;34m(mask, num_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindex_numpy_to_one_hot_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \"\"\"\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "processor = InferenceCore(network, config=config)\n",
        "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
        "cap = cv2.VideoCapture(video_name)\n",
        "\n",
        "# You can change these two numbers\n",
        "frames_to_propagate = 200\n",
        "visualize_every = 20\n",
        "\n",
        "current_frame_index = 0\n",
        "\n",
        "with torch.cuda.amp.autocast(enabled=True):\n",
        "  while (cap.isOpened()):\n",
        "    # load frame-by-frame\n",
        "    _, frame = cap.read()\n",
        "    if frame is None or current_frame_index > frames_to_propagate:\n",
        "      break\n",
        "\n",
        "    # convert numpy array to pytorch tensor format\n",
        "    frame_torch, _ = image_to_torch(frame, device=device)\n",
        "    if current_frame_index == 0:\n",
        "      # initialize with the mask\n",
        "      mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
        "      # the background mask is not fed into the model\n",
        "      prediction = processor.step(frame_torch, mask_torch[1:])\n",
        "    else:\n",
        "      # propagate only\n",
        "      prediction = processor.step(frame_torch)\n",
        "\n",
        "\n",
        "    # print(\"elso\",prediction)\n",
        "    # argmax, convert to numpy\n",
        "    prediction = torch_prob_to_numpy_mask(prediction)\n",
        "    # x_s, y_s = prediction.shape\n",
        "    # for x in range(x_s):\n",
        "    #   for y in range(y_s):\n",
        "    #     print(prediction[x,y], end=\"\")\n",
        "    #   print(end=\"\\n\")\n",
        "\n",
        "    a,b,c,d = None, None, None, None\n",
        "    permit = math.inf\n",
        "    for obj_id in np.unique(mask)[1:]:\n",
        "      first_occurrence_idx = np.argmax(prediction == obj_id)\n",
        "      first_occurrence_coordinates = np.unravel_index(first_occurrence_idx, prediction.shape)\n",
        "\n",
        "      # Find the (x, y) coordinates of the last occurrence of 1\n",
        "      last_occurrence_idx = np.argwhere(prediction == obj_id)[-1]\n",
        "      last_occurrence_coordinates = tuple(last_occurrence_idx)\n",
        "\n",
        "      # so far so good\n",
        "\n",
        "      coordinates = np.argwhere(prediction == obj_id)\n",
        "\n",
        "      min_y_idx_in_coordinates = np.argmin(coordinates[:, 1])\n",
        "      max_y_idx_in_coordinates = np.argmax(coordinates[:, 1])\n",
        "\n",
        "      min_y_element = coordinates[min_y_idx_in_coordinates]\n",
        "      max_y_element = coordinates[max_y_idx_in_coordinates]\n",
        "\n",
        "      # Calculate the distances from the 0-th index\n",
        "\n",
        "      # print(\"id\", obj_id)\n",
        "      # print(f\"First occurrence coordinates: {first_occurrence_coordinates}\")\n",
        "      # print(f\"Last occurrence coordinates: {last_occurrence_coordinates}\")\n",
        "\n",
        "      # print(\"Smallest y element:\", min_y_element)\n",
        "      # print(\"Largest y element:\", max_y_element)\n",
        "      quadrilateral = perimeter_of_quadrilateral(first_occurrence_coordinates, last_occurrence_coordinates, min_y_element, max_y_element)\n",
        "      # print(\"permiter\", quadrilateral)\n",
        "\n",
        "      if quadrilateral < permit:\n",
        "        a,b,c,d = first_occurrence_coordinates, last_occurrence_coordinates, min_y_element, max_y_element\n",
        "        permit = quadrilateral\n",
        "      # print(a,b,c,d)\n",
        "    max_x = max(a[0], b[0], c[0], d[0])\n",
        "    max_y = max(a[1], b[1], c[1], d[1])\n",
        "    min_x = min(a[0], b[0], c[0], d[0])\n",
        "    min_y = min(a[1], b[1], c[1], d[1])\n",
        "      # print(tuple([min_x,min_y]), tuple([max_x,max_y]))\n",
        "      # print(tuple([min_x,max_y]),tuple([max_x,min_y]))\n",
        "    # print(tuple([max_y, min_x]),tuple([min_y, max_x]))\n",
        "    frame_bbox = cv2.rectangle(frame,tuple([max_y, min_x]),tuple([min_y, max_x]), (0, 255, 0), 2)\n",
        "      \n",
        "    if current_frame_index % visualize_every == 0:\n",
        "      visualization = overlay_davis(frame_bbox, prediction)\n",
        "      display(Image.fromarray(visualization))\n",
        "\n",
        "    current_frame_index += 1\n",
        "    # break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
